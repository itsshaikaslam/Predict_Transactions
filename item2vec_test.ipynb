{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3063: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/cc_data.csv')\n",
    "df = df[-100000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records used: 99753\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Transaction ID</th>\n",
       "      <th>Consumer ID</th>\n",
       "      <th>Normalized Retailer</th>\n",
       "      <th>SIC Description</th>\n",
       "      <th>Purchase Amount</th>\n",
       "      <th>Transaction Date</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Transaction Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-11-11</th>\n",
       "      <td>5c1178fc7626b91e0173f4a7</td>\n",
       "      <td>CLYFgsfxk1Lu2A6bbFw5Ig==\\n</td>\n",
       "      <td>Tim Hortons</td>\n",
       "      <td>Eating Places</td>\n",
       "      <td>5.02</td>\n",
       "      <td>2018-11-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-21</th>\n",
       "      <td>5d61c1d8c6cd34706e379be1</td>\n",
       "      <td>OY1aqwp53P2m3WeMp31SYA==\\n</td>\n",
       "      <td>Save On Foods</td>\n",
       "      <td>Grocery Stores</td>\n",
       "      <td>205.05</td>\n",
       "      <td>2019-08-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-14</th>\n",
       "      <td>5d3486418f5c3f7071751711</td>\n",
       "      <td>EyrP3XDsGZMZMjjQVW7W4A==\\n</td>\n",
       "      <td>Esso</td>\n",
       "      <td>Gasoline Service Stations</td>\n",
       "      <td>62.69</td>\n",
       "      <td>2019-07-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-31</th>\n",
       "      <td>5dc16c551829ca6e7a2cfa46</td>\n",
       "      <td>q+IvLS99GQ2iD1mtk8lwVQ==\\n</td>\n",
       "      <td>Domino's Pizza</td>\n",
       "      <td>Eating Places</td>\n",
       "      <td>15.18</td>\n",
       "      <td>2019-10-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-01</th>\n",
       "      <td>5d1ded93fc71c0707cce6bc8</td>\n",
       "      <td>0mEZrMNV8K/BJxFE2HXjuw==\\n</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Book Stores</td>\n",
       "      <td>24.35</td>\n",
       "      <td>2019-07-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Transaction ID                 Consumer ID  \\\n",
       "Transaction Date                                                         \n",
       "2018-11-11        5c1178fc7626b91e0173f4a7  CLYFgsfxk1Lu2A6bbFw5Ig==\\n   \n",
       "2019-08-21        5d61c1d8c6cd34706e379be1  OY1aqwp53P2m3WeMp31SYA==\\n   \n",
       "2019-07-14        5d3486418f5c3f7071751711  EyrP3XDsGZMZMjjQVW7W4A==\\n   \n",
       "2019-10-31        5dc16c551829ca6e7a2cfa46  q+IvLS99GQ2iD1mtk8lwVQ==\\n   \n",
       "2019-07-01        5d1ded93fc71c0707cce6bc8  0mEZrMNV8K/BJxFE2HXjuw==\\n   \n",
       "\n",
       "                 Normalized Retailer            SIC Description  \\\n",
       "Transaction Date                                                  \n",
       "2018-11-11               Tim Hortons              Eating Places   \n",
       "2019-08-21             Save On Foods             Grocery Stores   \n",
       "2019-07-14                      Esso  Gasoline Service Stations   \n",
       "2019-10-31            Domino's Pizza              Eating Places   \n",
       "2019-07-01                    Amazon                Book Stores   \n",
       "\n",
       "                  Purchase Amount Transaction Date  \n",
       "Transaction Date                                    \n",
       "2018-11-11                   5.02       2018-11-11  \n",
       "2019-08-21                 205.05       2019-08-21  \n",
       "2019-07-14                  62.69       2019-07-14  \n",
       "2019-10-31                  15.18       2019-10-31  \n",
       "2019-07-01                  24.35       2019-07-01  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[['Transaction ID','Consumer ID','Normalized Retailer','SIC Description','Purchase Amount','Transaction Date']]\n",
    "df['Transaction Date'].replace({\"N\\A\":None}, inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "df.index = pd.to_datetime(df['Transaction Date']) # Use date as index\n",
    "print(\"Total records used: %d\" % df.shape[0])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tim Hortons', 'Save On Foods', 'Esso', \"Domino's Pizza\", 'Amazon', 'Rona', 'Pizzaville', 'Uber', 'Lyft', 'Patreon', 'Petro-Canada', 'A&W', 'IKEA', 'The Beer Store', '7 Eleven', 'No Frills', 'Loblaws', 'Best Buy', 'Indigo', \"McDonald's\"]\n",
      "Numbers of retailers: 1409\n"
     ]
    }
   ],
   "source": [
    "retailer_list = list(df['Normalized Retailer'].unique())\n",
    "print(retailer_list[:20])\n",
    "print(\"Numbers of retailers: %d\" % len(retailer_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Eating Places', 'Grocery Stores', 'Gasoline Service Stations', 'Book Stores', 'Hardware Stores', 'Taxicabs', 'Bands, Orchestras, Actors, and Other Entertainers and Entertainment Groups', 'Furniture Stores', 'Liquor Stores', 'Family Clothing Stores']\n",
      "Numbers of Sectors: 99\n"
     ]
    }
   ],
   "source": [
    "SIC_list = list(df['SIC Description'].unique())\n",
    "print(SIC_list[:10])\n",
    "print(\"Numbers of Sectors: %d\" % len(SIC_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary from SIC to retailer\n",
    "retailer_map = defaultdict(list) # DO NOT USE dict.fromkeys, which append retailer to every key\n",
    "\n",
    "for i in range(len(retailer_list)):\n",
    "    tmp_SIC = df.loc[df['Normalized Retailer'] == retailer_list[i]]['SIC Description'][0] \n",
    "    retailer_map[tmp_SIC].append(retailer_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Beer Store',\n",
       " 'Bc Liquor Stores',\n",
       " \"Claire's\",\n",
       " 'Wine Rack',\n",
       " 'Vineyards',\n",
       " 'Saint Laurent',\n",
       " 'Bench',\n",
       " 'Solo Liquor Store',\n",
       " 'Econo Lodge']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view selected key-values from the dictionary\n",
    "#dict(list(retailer_map.items())[7: 9])\n",
    "retailer_map['Liquor Stores']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create retailer sets grouped by SIC\n",
    "def groupBySIC(df):\n",
    "    df_group = df.groupby('SIC Description').agg(lambda x: x.unique().tolist())\n",
    "    training_data = df_group['Normalized Retailer'].tolist()\n",
    "    training_data = [x for x in training_data if len(x) > 1] # remove sets containing only one retailer\n",
    "    return training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create retailer sets grouped by individual and shopping frequency\n",
    "# 1, group by daily shopping > day_threshold\n",
    "# 2, discard records used in step 1\n",
    "# 3, group by weekly shopping > week_threshold\n",
    "# 4, put two groups together\n",
    "\n",
    "def groupByPersonAndTime(df,day_threshold = 2,week_threshold = 2):\n",
    "    \n",
    "    # create retailer sets grouped by customer and daily shopping\n",
    "    df_groupbyday = df.groupby(['Consumer ID',pd.Grouper(freq = 'D')]).agg(lambda x: x.tolist())\n",
    "    \n",
    "    # only keep sets above threshold\n",
    "    setbyday = df_groupbyday['Normalized Retailer'].tolist()\n",
    "    setbyday = [x for x in setbyday if len(x) >= day_threshold]    \n",
    "    print(\"Records percentage used from daily group: %.2f\" % (sum([len(listElem) for listElem in setbyday])/df.shape[0]*100))\n",
    "    \n",
    "    # Transaction ID records not used yet\n",
    "    tmp = df_groupbyday['Transaction ID'].tolist()\n",
    "    tmp = [x for x in tmp if len(x) < day_threshold] \n",
    "    IDs2keep = [item for sublist in tmp for item in sublist]\n",
    "    \n",
    "    # df without records used in previous step\n",
    "    df_removedays = df[df['Transaction ID'].isin(IDs2keep)]\n",
    "    \n",
    "    # create retailer sets grouped by customer and weekly shopping\n",
    "    df_groupbyweek = df_removedays.groupby(['Consumer ID',pd.Grouper(freq = 'W')]).agg(lambda x: x.tolist())\n",
    "    \n",
    "    # only keep sets above threshold\n",
    "    setbyweek = df_groupbyweek['Normalized Retailer'].tolist()\n",
    "    setbyweek = [x for x in setbyweek if len(x) >= week_threshold] \n",
    "    print(\"Records percentage used from weekly group: %.2f\" % (sum([len(listElem) for listElem in setbyweek])/df.shape[0]*100))\n",
    "    \n",
    "    return setbyday + setbyweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records percentage used from daily group: 1.10\n"
     ]
    }
   ],
   "source": [
    "training_data = groupByPersonAndTime(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.datetime.now()\n",
    "\n",
    "model = Word2Vec(sentences = training_data, # list of sets of retailers\n",
    "                 iter = 5, # epoch\n",
    "                 min_count = 1, # a retailer has to appear more than min_count times to be kept\n",
    "                 size = 10, # hidden layer dimensions\n",
    "                 workers = 4, # specify the number of threads to be used for training\n",
    "                 sg = 1, # Defines the training algorithm. We will use skip-gram so 1 is chosen.\n",
    "                 hs = 0, # Set to 0, as we are applying negative sampling.\n",
    "                 negative = 5, # If > 0, negative sampling will be used. We will use a value of 5.\n",
    "                 window = 9999999)\n",
    "\n",
    "print(\"Model training time: \" + str(datetime.datetime.now()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check dictionary\n",
    "for i, word in enumerate(model.wv.vocab):\n",
    "    if i == 10:\n",
    "        break\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check vector given a certain word from the dictionary\n",
    "print(model.wv['Amazon'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate similarity between two words\n",
    "pairs = [\n",
    "    (\"Netflix\", \"Rogers\"), \n",
    "    (\"Netflix\", \"Starbucks\"),   \n",
    "    (\"Tim Hortons\", \"Starbucks\"), \n",
    "]\n",
    "print(\"Similarity comparison\\n\")\n",
    "for w1, w2 in pairs:\n",
    "    print('%r\\t%r\\t%.2f' % (w1, w2, model.wv.similarity(w1, w2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find out most similar terms\n",
    "print(model.wv.most_similar(positive=['Amazon'], topn=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize all words\n",
    "from sklearn.decomposition import IncrementalPCA    # inital reduction\n",
    "from sklearn.manifold import TSNE                   # final reduction\n",
    "\n",
    "def reduce_dimensions(model):\n",
    "    num_dimensions = 2  # final num dimensions (2D, 3D, etc)\n",
    "\n",
    "    vectors = [] # positions in vector space\n",
    "    labels = [] # keep track of words to label our data again later\n",
    "    for word in model.wv.vocab:\n",
    "        vectors.append(model.wv[word])\n",
    "        labels.append(word)\n",
    "\n",
    "    # convert both lists into numpy vectors for reduction\n",
    "    vectors = np.asarray(vectors)\n",
    "    labels = np.asarray(labels)\n",
    "\n",
    "    # reduce using t-SNE\n",
    "    vectors = np.asarray(vectors)\n",
    "    tsne = TSNE(n_components=num_dimensions, random_state=0)\n",
    "    vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "    x_vals = [v[0] for v in vectors]\n",
    "    y_vals = [v[1] for v in vectors]\n",
    "    return x_vals, y_vals, labels\n",
    "\n",
    "x_vals, y_vals, labels = reduce_dimensions(model)\n",
    "\n",
    "def plot_with_matplotlib(x_vals, y_vals, labels):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import random\n",
    "\n",
    "    random.seed(0)\n",
    "\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.scatter(x_vals, y_vals)\n",
    "\n",
    "    #\n",
    "    # Label randomly subsampled 25 data points\n",
    "    #\n",
    "    indices = list(range(len(labels)))\n",
    "    selected_indices = random.sample(indices, 25)\n",
    "    for i in selected_indices:\n",
    "        plt.annotate(labels[i], (x_vals[i], y_vals[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_with_matplotlib(x_vals, y_vals, labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
